# -*- coding: utf-8 -*-
# crawl_refactor/interfaces.py

from __future__ import annotations

from typing import Any, Dict, Optional, Protocol, Sequence, TYPE_CHECKING

if TYPE_CHECKING:
    from .models import NormalizedItem


class BrowserController(Protocol):
    async def get(self, url: str) -> None: ...
    async def scroll_bottom(self) -> None: ...
    async def wait_css(self, selector: str, timeout: float, *, visible: bool = False) -> None: ...
    async def wait_xpath(self, selector: str, timeout: float, *, visible: bool = False) -> None: ...
    async def get_dom(self) -> str: ...
    async def execute_js(self, script: str) -> Any: ...


class Navigator(Protocol):
    async def load(self, base_url: str, query: Optional[str] = None, params: Optional[Dict] = None) -> str: ...
    async def paginate(self, page: int) -> str: ...
    async def scroll(self, strategy: Any, max_scrolls: int, pause_sec: float) -> None: ...
    async def wait(self, hook: Any, selector: Optional[str], timeout: float, condition: str) -> None: ...
    async def get_dom(self) -> str: ...
    async def execute_js(self, script: str) -> Any: ...


class ResourceFetcher(Protocol):
    async def fetch_json(self, url: str, *, method: str = "GET", params: Optional[Dict] = None, payload: Optional[Dict] = None) -> Dict[str, Any]: ...
    async def fetch_bytes(self, url: str, *, method: str = "GET") -> bytes: ...


class CrawlSaver(Protocol):
    async def save_many(self, items: Sequence["NormalizedItem"], fetcher: Optional[ResourceFetcher] = None) -> Any: ...


class ExtractorBase(Protocol):
    async def extract(self) -> list[Dict[str, Any]]: ...
