"""
crawl.py
---------
CrawlRunner
 - URLë§Œ ì´ˆê¸°í™” ì‹œ ì£¼ì…
 - run(cfg_like | yamlPath) ì‹œì ì— ì •ì±… ë¡œë“œ ë° ë³‘í•© ìˆ˜í–‰
"""

import asyncio
from pathlib import Path
from cfg_utils import PolicyLoader
from logs_utils import get_session_logger

from crawl_refactor.policy import CrawlPolicy
from crawl_refactor.pipeline import CrawlPipeline
from crawl_refactor.navigator import SeleniumNavigator
from crawl_refactor.fetcher import HTTPFetcher
from crawl_refactor.normalizer import DataNormalizer
from crawl_refactor.saver import StorageDispatcher


class CrawlRunner:
    """
    CrawlPipeline ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
    firefox.yaml (ê¸°ë³¸) + ì‚¬ìš©ì ì •ì±…(cfg_like ë˜ëŠ” yamlPath)ì„ ë³‘í•© í›„ ì‹¤í–‰
    """

    def __init__(self, url: str, firefox_yaml: str = "firefox.yaml"):
        self.url = url
        self.firefox_yaml = firefox_yaml
        self.logger = get_session_logger(Path(url).stem or "crawl_session")

    async def run(self, cfg_like: dict | None = None, yamlPath: str | None = None):
        """
        cfg_like: dict í˜•íƒœì˜ ì •ì±… (ì˜ˆ: API í˜¸ì¶œ ë“±ì—ì„œ ì „ë‹¬)
        yamlPath: ì‚¬ìš©ì ì§€ì • YAML ê²½ë¡œ (ì˜ˆ: crawl.yaml)
        ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì…ë ¥í•˜ë©´ ëœë‹¤.
        """
        # -------------------
        # 1. ì •ì±… ë¡œë“œ ë° ë³‘í•©
        # -------------------
        if cfg_like and yamlPath:
            raise ValueError("cfg_likeì™€ yamlPathëŠ” ë™ì‹œì— ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if cfg_like:
            self.logger.info("ğŸ§© merging firefox.yaml with dict override")
            merged_policy = PolicyLoader.load(
                base=self.firefox_yaml, override_dict=cfg_like, model=CrawlPolicy
            )
        elif yamlPath:
            self.logger.info("ğŸ§© merging firefox.yaml with YAML override: {}", yamlPath)
            merged_policy = PolicyLoader.load(
                base=self.firefox_yaml, override=yamlPath, model=CrawlPolicy
            )
        else:
            self.logger.info("ğŸ§© loading firefox.yaml only (no override)")
            merged_policy = PolicyLoader.load(base=self.firefox_yaml, model=CrawlPolicy)

        # URL override (ìµœì¢… ì •ì±…ì— URL ì£¼ì…)
        merged_policy.navigation.base_url = self.url
        self.policy = merged_policy

        # -------------------
        # 2. êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        # -------------------
        self.logger.info("ğŸš€ Starting CrawlPipeline for {}", self.url)
        navigator = SeleniumNavigator(policy=self.policy)
        fetcher = HTTPFetcher(policy=self.policy)
        normalizer = DataNormalizer(policy=self.policy)
        saver = StorageDispatcher(policy=self.policy)

        pipeline = CrawlPipeline(
            policy=self.policy,
            navigator=navigator,
            fetcher=fetcher,
            normalizer=normalizer,
            saver=saver,
        )

        # -------------------
        # 3. ì‹¤í–‰
        # -------------------
        try:
            await pipeline.run()
            self.logger.info("âœ… CrawlPipeline completed successfully.")
        except Exception as e:
            self.logger.exception("âŒ CrawlPipeline failed: {}", e)
        finally:
            self.logger.info("ğŸ“¦ Output saved in: {}", self.policy.storage.base_dir)


# ----------------------------
# CLI
# ----------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Run a single crawl task (cfg_like or YAML override).")
    parser.add_argument("--url", required=True, help="Target URL to crawl")
    parser.add_argument("--yaml", help="Path to crawl YAML config (optional)")
    parser.add_argument("--firefox", default="firefox.yaml", help="Path to firefox.yaml")

    args = parser.parse_args()

    runner = CrawlRunner(args.url, firefox_yaml=args.firefox)
    asyncio.run(runner.run(yamlPath=args.yaml))
