# xlcrawl2.py ì™„ì „ êµ¬í˜„ì„ ìœ„í•œ ê°œì„  ê³„íš

## ğŸ“‹ í˜„ì¬ ìƒíƒœ ë¶„ì„

### âœ… ì™„ë£Œëœ ë¶€ë¶„
1. **í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”**
   - âœ… `load_paths_config()` - paths.local.yaml ë¡œë“œ
   - âœ… `get_config_paths()` - excel.yaml, xlcrawl.yaml ê²½ë¡œ ì¶”ì¶œ
   - âœ… ConfigLoader + BaseParserPolicy í†µí•©

2. **Excel ì²˜ë¦¬**
   - âœ… `XlController` - Excel íŒŒì¼ ì—´ê¸°
   - âœ… `to_dataframe()` - DataFrame ì¶”ì¶œ
   - âœ… `extract_urls_from_dataframe()` - URL ì¶”ì¶œ
   - âœ… `update_download_column()` - ë‚ ì§œ ê¸°ì…

3. **WebDriver ì„¤ì •**
   - âœ… Firefox WebDriver ì •ì±… (xlcrawl.yaml)
   - âœ… `create_webdriver("firefox", config)` - WebDriver ìƒì„±

### âš ï¸ ë¯¸ì™„ì„± ë¶€ë¶„ (process_crawling í•¨ìˆ˜)

```python
# í˜„ì¬ êµ¬í˜„ (process_crawling í•¨ìˆ˜ ë‚´ë¶€)
driver.driver.get(url)  # âœ… í˜ì´ì§€ ë¡œë“œë§Œ ë¨

# TODO: ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§
# - JS extract ì‹¤í–‰
# - ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
# - ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
```

---

## ğŸ¯ ê°œì„ ì´ í•„ìš”í•œ Utils ë° Adapter

### 1. **crawl_utils ê°œì„  ì‚¬í•­**

#### 1.1 DetailEntryPoint í™œìš© (ì¶”ì²œ)
**ìœ„ì¹˜**: `modules/crawl_utils/services/entry_points/detail.py`

**í˜„ì¬ ìƒíƒœ**:
- âœ… ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ì¡´ì¬
- âœ… Navigator, SmartNormalizer, FileSaver í†µí•©
- âš ï¸ **async ì „ìš©** - xlcrawl2.pyëŠ” sync í™˜ê²½

**ê°œì„  ë°©ì•ˆ**:
```python
# Option 1: SyncCrawlRunner ì‚¬ìš©
from crawl_utils import SyncCrawlRunner, DetailEntryPoint

runner = SyncCrawlRunner(DetailEntryPoint(navigator, policy))
result = runner.run(url="https://example.com/product/123")

# Option 2: asyncio.run() ë˜í•‘
import asyncio
entry_point = DetailEntryPoint(navigator, policy)
result = asyncio.run(entry_point.run(url))
```

**í•„ìš”í•œ ì‘ì—…**:
- [ ] `SyncCrawlRunner` + `DetailEntryPoint` í†µí•© í…ŒìŠ¤íŠ¸
- [ ] xlcrawl2.pyì—ì„œ ì‚¬ìš© ì˜ˆì œ ì¶”ê°€

---

#### 1.2 Navigator ë™ê¸°í™” ë˜í¼
**ìœ„ì¹˜**: `modules/crawl_utils/services/navigator.py`

**í˜„ì¬ ìƒíƒœ**:
```python
class SeleniumNavigator:
    async def load(self, url: str) -> None: ...
    async def wait(self, hook, selector, timeout, condition) -> None: ...
    async def execute_js(self, script: str) -> Any: ...
```

**ë¬¸ì œì **:
- xlcrawl2.pyëŠ” sync í™˜ê²½ (`with XlController` ì‚¬ìš©)
- DetailEntryPointëŠ” async ì „ìš©

**ê°œì„  ë°©ì•ˆ**:
```python
# ìƒˆ íŒŒì¼: modules/crawl_utils/services/sync_navigator.py
class SyncSeleniumNavigator:
    """ë™ê¸° í™˜ê²½ì„ ìœ„í•œ Navigator ë˜í¼"""
    
    def __init__(self, driver):
        self.driver = driver
    
    def load(self, url: str) -> None:
        self.driver.get(url)
    
    def wait(self, hook, selector, timeout, condition) -> None:
        # WebDriverWait ì‚¬ìš©
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.common.by import By
        
        if hook == "css":
            by = By.CSS_SELECTOR
        elif hook == "xpath":
            by = By.XPATH
        else:
            return
        
        wait = WebDriverWait(self.driver, timeout)
        if condition == "visibility":
            wait.until(EC.visibility_of_element_located((by, selector)))
        else:
            wait.until(EC.presence_of_element_located((by, selector)))
    
    def execute_js(self, script: str) -> Any:
        return self.driver.execute_script(script)
```

**í•„ìš”í•œ ì‘ì—…**:
- [ ] `SyncSeleniumNavigator` í´ë˜ìŠ¤ ìƒì„±
- [ ] `SyncDetailEntryPoint` ë˜ëŠ” ê¸°ì¡´ DetailEntryPointì—ì„œ async ì œê±°
- [ ] xlcrawl2.pyì—ì„œ ì‚¬ìš©

---

#### 1.3 JS Extract Snippet ê´€ë¦¬
**ìœ„ì¹˜**: `configs/xlcrawl.yaml` + ìƒˆ íŒŒì¼ í•„ìš”

**í˜„ì¬ ìƒíƒœ**:
```yaml
# xlcrawl.yaml
extract:
  patterns:  # XPath íŒ¨í„´ (DOM ì¶”ì¶œìš©)
    title: "//h1[@class='product-title']"
    images: "//img[@class='product-image']/@src"
```

**ë¬¸ì œì **:
- DetailEntryPointëŠ” `ExtractorType.JS` + `js_snippet` í•„ìš”
- í˜„ì¬ patternsëŠ” XPath (DOM ì¶”ì¶œìš©)

**ê°œì„  ë°©ì•ˆ**:
```yaml
# xlcrawl.yaml ê°œì„ 
extract:
  type: "js"  # "dom", "js", "api"
  
  # JS snippet ì§ì ‘ ì •ì˜
  js_snippet: |
    return {
      title: document.querySelector('h1.product-title')?.innerText,
      price: document.querySelector('span.price')?.innerText,
      images: Array.from(document.querySelectorAll('img.product-image')).map(img => img.src),
      description: document.querySelector('div.description')?.innerHTML
    };
  
  # ë˜ëŠ” JS íŒŒì¼ ê²½ë¡œ
  js_file: "scripts/extract_taobao.js"
```

**í•„ìš”í•œ ì‘ì—…**:
- [ ] xlcrawl.yamlì— `extract.type`, `extract.js_snippet` ì¶”ê°€
- [ ] ë˜ëŠ” ë³„ë„ JS íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ (scripts/extractors/*.js)
- [ ] DetailEntryPointì—ì„œ js_snippet ë¡œë“œ

---

### 2. **image_utils ê°œì„  ì‚¬í•­**

#### 2.1 ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
**ìœ„ì¹˜**: `modules/image_utils/` (ìƒˆ ëª¨ë“ˆ í•„ìš”)

**í˜„ì¬ ìƒíƒœ**:
- âœ… `ImageLoader` - ë¡œì»¬ ì´ë¯¸ì§€ ë¡œë“œ/ë³µì‚¬/ë¦¬ì‚¬ì´ì¦ˆ
- âœ… `ImageOCR` - OCR ì²˜ë¦¬
- âœ… `ImageOverlay` - í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´
- âŒ **URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì—†ìŒ**

**í•„ìš”í•œ ê¸°ëŠ¥**:
```python
# ìƒˆ íŒŒì¼: modules/image_utils/services/image_downloader.py
class ImageDownloader:
    """URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    
    def __init__(self, policy: ImageDownloadPolicy):
        self.policy = policy
    
    def download(self, url: str, save_path: Path) -> Path:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        response = requests.get(url, timeout=self.policy.timeout)
        response.raise_for_status()
        
        save_path.parent.mkdir(parents=True, exist_ok=True)
        save_path.write_bytes(response.content)
        
        return save_path
    
    def download_many(self, urls: List[str], save_dir: Path) -> List[Path]:
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        results = []
        for i, url in enumerate(urls):
            filename = f"image_{i:04d}{self._get_extension(url)}"
            save_path = save_dir / filename
            try:
                self.download(url, save_path)
                results.append(save_path)
            except Exception as e:
                print(f"Download failed: {url} - {e}")
        return results
```

**í•„ìš”í•œ ì‘ì—…**:
- [ ] `ImageDownloadPolicy` ì •ì±… ìƒì„±
- [ ] `ImageDownloader` í´ë˜ìŠ¤ êµ¬í˜„
- [ ] `image_utils/__init__.py`ì— export ì¶”ê°€
- [ ] xlcrawl.yamlì— download ì •ì±… ì¶”ê°€

---

#### 2.2 crawl_utils.FileSaverì™€ í†µí•©
**ìœ„ì¹˜**: `modules/crawl_utils/services/saver.py`

**í˜„ì¬ ìƒíƒœ**:
```python
class FileSaver:
    async def save_many(self, items: List[NormalizedItem], fetcher: ResourceFetcher) -> SaveSummary:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ + ì €ì¥"""
        # fetcher.fetch(url) ì‚¬ìš©
```

**ê°œì„  ë°©ì•ˆ**:
```python
# xlcrawl2.pyì—ì„œ FileSaver ì§ì ‘ ì‚¬ìš©
from crawl_utils.services.saver import FileSaver
from crawl_utils.services.fetcher import HTTPFetcher

fetcher = HTTPFetcher()
saver = FileSaver(storage_policy)

# NormalizedItemìœ¼ë¡œ ë³€í™˜ í•„ìš”
items = [
    NormalizedItem(
        kind="image",
        url=img_url,
        section="product_123",
        name="image_001",
        extension="jpg"
    )
    for img_url in image_urls
]

# ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ
import asyncio
summary = asyncio.run(saver.save_many(items, fetcher))
```

**í•„ìš”í•œ ì‘ì—…**:
- [ ] xlcrawl2.pyì—ì„œ NormalizedItem ìƒì„± ë¡œì§ ì¶”ê°€
- [ ] FileSaver ì‚¬ìš© ì˜ˆì œ ì¶”ê°€
- [ ] ë˜ëŠ” ë™ê¸° ë²„ì „ `SyncFileSaver` ìƒì„±

---

### 3. **xl_utils ê°œì„  ì‚¬í•­ (ì„ íƒ)**

#### 3.1 DataFrameì—ì„œ ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
**ìœ„ì¹˜**: `scripts/xlcrawl2.py` (update_download_column í•¨ìˆ˜)

**í˜„ì¬ ë¬¸ì œ**:
```python
col_idx = df.columns.get_loc(download_col) + 1  # ì˜¤ë¥˜ ê°€ëŠ¥ì„±
# get_loc()ì€ int | slice | ndarray ë°˜í™˜ ê°€ëŠ¥
```

**ê°œì„  ë°©ì•ˆ**:
```python
def get_excel_column_index(df: pd.DataFrame, col_name: str) -> int:
    """DataFrame ì»¬ëŸ¼ â†’ Excel ì»¬ëŸ¼ ì¸ë±ìŠ¤ (1-based)"""
    loc = df.columns.get_loc(col_name)
    
    if isinstance(loc, int):
        return loc + 1
    elif isinstance(loc, slice):
        return loc.start + 1
    else:
        # ndarrayì¸ ê²½ìš° ì²« ë²ˆì§¸ True ì¸ë±ìŠ¤
        return int(loc.argmax()) + 1
```

**í•„ìš”í•œ ì‘ì—…**:
- [ ] `xl_utils/helpers/` ì— ìœ í‹¸ í•¨ìˆ˜ ì¶”ê°€
- [ ] xlcrawl2.pyì—ì„œ ì‚¬ìš©

---

## ğŸ“ xlcrawl2.py ì™„ì „ êµ¬í˜„ ì‹œë‚˜ë¦¬ì˜¤

### Scenario A: DetailEntryPoint + SyncCrawlRunner (ì¶”ì²œ)

```python
# scripts/xlcrawl2.py
from crawl_utils import create_webdriver, SyncCrawlRunner, CrawlPolicy
from crawl_utils.services.entry_points import DetailEntryPoint
from crawl_utils.services.sync_navigator import SyncSeleniumNavigator

def process_crawling(urls, crawl_config, output_dir):
    # 1. WebDriver ìƒì„±
    driver = create_webdriver("firefox", crawl_config["firefox"])
    
    # 2. Navigator ìƒì„±
    navigator = SyncSeleniumNavigator(driver.driver)
    
    # 3. CrawlPolicy ìƒì„±
    policy = CrawlPolicy.from_dict(crawl_config["crawl_policy"])
    
    # 4. DetailEntryPoint ìƒì„±
    entry_point = DetailEntryPoint(navigator, policy)
    
    # 5. SyncCrawlRunnerë¡œ ì‹¤í–‰
    runner = SyncCrawlRunner(entry_point)
    
    results = {}
    for idx, url in urls:
        try:
            summary = runner.run(url, product_id=f"product_{idx}")
            results[idx] = {
                "status": "success",
                "images": len(summary.artifacts.get("image", [])),
                "texts": len(summary.artifacts.get("text", []))
            }
        except Exception as e:
            results[idx] = {"status": "failed", "error": str(e)}
    
    driver.quit()
    return results
```

**ì¥ì **:
- âœ… ê¸°ì¡´ crawl_utils ì¸í”„ë¼ ìµœëŒ€ í™œìš©
- âœ… SmartNormalizer ìë™ íƒ€ì… ì¶”ë¡ 
- âœ… FileSaver ìë™ ì €ì¥

**í•„ìš”í•œ ì‘ì—…**:
- [ ] `SyncSeleniumNavigator` êµ¬í˜„
- [ ] `SyncCrawlRunner` + `DetailEntryPoint` í†µí•© í™•ì¸
- [ ] xlcrawl.yamlì— CrawlPolicy ì„¹ì…˜ ì¶”ê°€

---

### Scenario B: ì§ì ‘ WebDriver + Manual Extract (ê°„ë‹¨)

```python
# scripts/xlcrawl2.py
from crawl_utils import create_webdriver
from image_utils import ImageDownloader

def process_crawling(urls, crawl_config, output_dir):
    # 1. WebDriver ìƒì„±
    driver = create_webdriver("firefox", crawl_config["firefox"])
    
    # 2. ImageDownloader ìƒì„±
    downloader = ImageDownloader(crawl_config["image_download"])
    
    results = {}
    for idx, url in urls:
        try:
            # í˜ì´ì§€ ë¡œë“œ
            driver.driver.get(url)
            
            # JS extract ì‹¤í–‰
            js_snippet = crawl_config["extract"]["js_snippet"]
            data = driver.driver.execute_script(js_snippet)
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            if "images" in data:
                save_dir = output_dir / f"product_{idx}"
                image_paths = downloader.download_many(data["images"], save_dir)
                
                results[idx] = {
                    "status": "success",
                    "title": data.get("title"),
                    "images": len(image_paths)
                }
        except Exception as e:
            results[idx] = {"status": "failed", "error": str(e)}
    
    driver.quit()
    return results
```

**ì¥ì **:
- âœ… êµ¬í˜„ ê°„ë‹¨
- âœ… ë™ê¸° í™˜ê²½ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

**ë‹¨ì **:
- âŒ SmartNormalizer, FileSaver ë“± ê¸°ì¡´ ì¸í”„ë¼ ë¯¸í™œìš©
- âŒ ìˆ˜ë™ êµ¬í˜„ í•„ìš”

**í•„ìš”í•œ ì‘ì—…**:
- [ ] `ImageDownloader` êµ¬í˜„
- [ ] xlcrawl.yamlì— `extract.js_snippet` ì¶”ê°€

---

## ğŸ¯ ìµœì¢… ìš°ì„ ìˆœìœ„

### Phase 1: ê¸°ë³¸ ë™ì‘ (Scenario B ê¸°ë°˜)
1. [ ] `ImageDownloader` êµ¬í˜„
   - `modules/image_utils/services/image_downloader.py`
   - `ImageDownloadPolicy` ì •ì±…
2. [ ] xlcrawl.yamlì— `js_snippet` ì¶”ê°€
3. [ ] xlcrawl2.pyì—ì„œ ì§ì ‘ WebDriver + JS extract
4. [ ] í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…

**ì˜ˆìƒ ì‹œê°„**: 2-3ì‹œê°„

---

### Phase 2: crawl_utils í†µí•© (Scenario A ê¸°ë°˜)
1. [ ] `SyncSeleniumNavigator` êµ¬í˜„
   - `modules/crawl_utils/services/sync_navigator.py`
2. [ ] `SyncDetailEntryPoint` ë˜ëŠ” ê¸°ì¡´ DetailEntryPoint ë™ê¸°í™”
3. [ ] xlcrawl.yamlì— CrawlPolicy ì„¹ì…˜ ì¶”ê°€
4. [ ] xlcrawl2.pyì—ì„œ DetailEntryPoint + SyncCrawlRunner ì‚¬ìš©
5. [ ] í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…

**ì˜ˆìƒ ì‹œê°„**: 4-6ì‹œê°„

---

### Phase 3: ê³ ë„í™”
1. [ ] JS extract snippet ì™¸ë¶€ íŒŒì¼ ê´€ë¦¬ (`scripts/extractors/*.js`)
2. [ ] ì¬ì‹œë„ ë¡œì§ ê°•í™”
3. [ ] ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ 
4. [ ] ë¡œê¹… ê°•í™” (logs_utils í†µí•©)
5. [ ] ì„±ëŠ¥ ìµœì í™” (ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ)

**ì˜ˆìƒ ì‹œê°„**: 4-8ì‹œê°„

---

## ğŸ“Œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… (Quick Wins)

### 1. xlcrawl.yamlì— js_snippet ì¶”ê°€
```yaml
extract:
  type: "js"
  js_snippet: |
    return {
      title: document.querySelector('h1.product-title')?.innerText || '',
      price: document.querySelector('span.price')?.innerText || '',
      images: Array.from(document.querySelectorAll('img.product-image')).map(img => img.src),
      description: document.querySelector('div.description')?.innerHTML || ''
    };
```

### 2. ImageDownloader ê¸°ë³¸ êµ¬í˜„
```python
# modules/image_utils/services/image_downloader.py
import requests
from pathlib import Path
from typing import List
from pydantic import BaseModel, Field

class ImageDownloadPolicy(BaseModel):
    timeout: int = Field(30, description="Download timeout (seconds)")
    max_retries: int = Field(3, description="Max retry count")
    user_agent: str = Field("Mozilla/5.0", description="User-Agent header")

class ImageDownloader:
    def __init__(self, policy: ImageDownloadPolicy | dict):
        if isinstance(policy, dict):
            policy = ImageDownloadPolicy(**policy)
        self.policy = policy
    
    def download(self, url: str, save_path: Path) -> Path:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        headers = {"User-Agent": self.policy.user_agent}
        response = requests.get(url, headers=headers, timeout=self.policy.timeout)
        response.raise_for_status()
        
        save_path.parent.mkdir(parents=True, exist_ok=True)
        save_path.write_bytes(response.content)
        
        return save_path
    
    def download_many(self, urls: List[str], save_dir: Path, prefix: str = "image") -> List[Path]:
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        results = []
        for i, url in enumerate(urls):
            ext = Path(url).suffix or ".jpg"
            filename = f"{prefix}_{i:04d}{ext}"
            save_path = save_dir / filename
            
            try:
                self.download(url, save_path)
                results.append(save_path)
                print(f"  âœ… Downloaded: {filename}")
            except Exception as e:
                print(f"  âŒ Failed: {url} - {e}")
        
        return results
```

### 3. xlcrawl2.py ê¸°ë³¸ í¬ë¡¤ë§ ë¡œì§
```python
# process_crawling í•¨ìˆ˜ ë‚´ë¶€ ìˆ˜ì •
def process_crawling(urls, crawl_config, output_dir):
    from image_utils.services.image_downloader import ImageDownloader, ImageDownloadPolicy
    
    # WebDriver ìƒì„±
    driver = create_webdriver("firefox", crawl_config["firefox"])
    
    # ImageDownloader ìƒì„±
    download_policy = ImageDownloadPolicy(
        timeout=crawl_config.get("crawl", {}).get("timeout", 30)
    )
    downloader = ImageDownloader(download_policy)
    
    # JS snippet ë¡œë“œ
    js_snippet = crawl_config.get("extract", {}).get("js_snippet", "return {};")
    
    results = {}
    for idx, url in urls:
        print(f"\nğŸŒ í¬ë¡¤ë§ ì¤‘ [{idx}]: {url}")
        
        try:
            # í˜ì´ì§€ ë¡œë“œ
            driver.driver.get(url)
            
            # JS extract ì‹¤í–‰
            data = driver.driver.execute_script(js_snippet)
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            images = data.get("images", [])
            if images:
                save_dir = output_dir / f"product_{idx}"
                image_paths = downloader.download_many(images, save_dir)
                
                results[idx] = {
                    "status": "success",
                    "title": data.get("title", ""),
                    "price": data.get("price", ""),
                    "images_count": len(image_paths),
                    "timestamp": datetime.now().isoformat()
                }
                print(f"  âœ… ì„±ê³µ: {data.get('title')} - {len(image_paths)}ê°œ ì´ë¯¸ì§€")
            else:
                results[idx] = {
                    "status": "success",
                    "title": data.get("title", ""),
                    "images_count": 0
                }
        
        except Exception as e:
            print(f"  âŒ ì‹¤íŒ¨: {e}")
            results[idx] = {"status": "failed", "error": str(e)}
    
    driver.quit()
    return results
```

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì‹¤í–‰:
1. **ImageDownloader êµ¬í˜„** â†’ `modules/image_utils/services/image_downloader.py`
2. **xlcrawl.yaml ì—…ë°ì´íŠ¸** â†’ `extract.js_snippet` ì¶”ê°€
3. **xlcrawl2.py ìˆ˜ì •** â†’ process_crawling í•¨ìˆ˜ ì™„ì„±
4. **í…ŒìŠ¤íŠ¸ ì‹¤í–‰** â†’ ì‹¤ì œ URLë¡œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸

### ì¥ê¸° ê³„íš:
- DetailEntryPoint + SyncCrawlRunner í†µí•©
- JS snippet ì™¸ë¶€ íŒŒì¼ ê´€ë¦¬
- ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ìµœì í™”
