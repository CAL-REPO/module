# -*- coding: utf-8 -*-
"""
scripts/xlcrawl2.py
Excel í¬ë¡¤ë§ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ (crawl_utils ê¸°ì¡´ êµ¬ì¡° í™œìš©)

ì›Œí¬í”Œë¡œìš°:
1. í™˜ê²½ë³€ìˆ˜(CASHOP_PATHS)ì—ì„œ paths.local.yaml ê²½ë¡œ íšë“
2. configs/excel.yaml, configs/xlcrawl.yaml ê²½ë¡œë¥¼ paths.local.yamlì—ì„œ ì°¸ì¡°
3. excel.yaml ì •ì±…ì— ë”°ë¼ xl_utilsë¡œ Excel íŒŒì¼ ì—´ê¸°
4. DataFrame ì¶”ì¶œ
5. download ì»¬ëŸ¼ì—ì„œ ë‚ ì§œê°€ ì•„ë‹Œ í–‰ â†’ URL ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
6. crawl_utils.CrawlPipelineìœ¼ë¡œ í¬ë¡¤ë§ ìˆ˜í–‰
7. download ì—´ì— ë‚ ì§œ ê¸°ì…
"""

import os
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Dict, Any
import pandas as pd

# PYTHONPATH ê¸°ì¤€ import
from xl_utils import XlController
from cfg_utils import ConfigLoader
from crawl_utils import CrawlPipeline, CrawlPolicy, StoragePolicy, FirefoxPolicy, create_webdriver
from crawl_utils.services.fetcher import HTTPFetcher
from crawl_utils.services.normalizer import DataNormalizer
from crawl_utils.services.saver import FileSaver
from structured_io.core.base_policy import BaseParserPolicy


def load_paths_config() -> dict:
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ paths.local.yaml ê²½ë¡œ íšë“ ë° ë¡œë“œ
    
    Returns:
        paths ì„¤ì • dict
    
    Raises:
        ValueError: í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°
        FileNotFoundError: paths.local.yaml íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
    """
    # 1. í™˜ê²½ë³€ìˆ˜ì—ì„œ ê²½ë¡œ íšë“
    paths_env = os.getenv("CASHOP_PATHS")
    
    if not paths_env:
        raise ValueError(
            "í™˜ê²½ë³€ìˆ˜ CASHOP_PATHSê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            "PowerShell: $env:CASHOP_PATHS = 'M:\\CALife\\CAShop - êµ¬ë§¤ëŒ€í–‰\\_code\\configs\\paths.local.yaml'"
        )
    
    paths_file = Path(paths_env)
    
    if not paths_file.exists():
        raise FileNotFoundError(f"paths.local.yaml íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {paths_file}")
    
    # 2. ConfigLoaderë¡œ ë¡œë“œ (placeholder í™œì„±í™”)
    yaml_policy = BaseParserPolicy(
        file_path=str(paths_file),
        enable_env=True,
        enable_placeholder=True,
        enable_reference=True,
        enable_include=False,
        default_section=None,
        encoding="utf-8",
        on_error="raise",
        safe_mode=True
    )
    loader = ConfigLoader(paths_file, yaml_policy=yaml_policy)
    paths_config = loader.as_dict()
    
    print(f"âœ… paths.local.yaml ë¡œë“œ ì™„ë£Œ: {paths_file}")
    return paths_config


def get_config_paths(paths_config: dict) -> Tuple[Path, Path]:
    """paths.local.yamlì—ì„œ excel.yaml, xlcrawl.yaml ê²½ë¡œ ì¶”ì¶œ
    
    Args:
        paths_config: paths.local.yamlì—ì„œ ë¡œë“œëœ ì„¤ì •
    
    Returns:
        (excel_config_path, xlcrawl_config_path)
    
    Raises:
        ValueError: configs ì„¹ì…˜ì´ ì—†ê±°ë‚˜ í•„ìˆ˜ í‚¤ê°€ ì—†ëŠ” ê²½ìš°
        FileNotFoundError: ì„¤ì • íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
    """
    # base_path ì¶”ì¶œ
    base_path = Path(paths_config.get("base_path", "M:/CALife/CAShop - êµ¬ë§¤ëŒ€í–‰/_code"))
    
    # configs ì„¹ì…˜ì—ì„œ ìƒëŒ€ ê²½ë¡œ ì¶”ì¶œ
    excel_yaml = base_path / paths_config.get("configs", {}).get("excel", "configs/excel.yaml")
    xlcrawl_yaml = base_path / paths_config.get("configs", {}).get("xlcrawl", "configs/xlcrawl.yaml")
    
    if not excel_yaml.exists():
        raise FileNotFoundError(f"excel.yamlì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {excel_yaml}")
    
    if not xlcrawl_yaml.exists():
        raise FileNotFoundError(f"xlcrawl.yamlì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {xlcrawl_yaml}")
    
    print(f"âœ… excel.yaml: {excel_yaml}")
    print(f"âœ… xlcrawl.yaml: {xlcrawl_yaml}")
    
    return excel_yaml, xlcrawl_yaml


def extract_urls_from_dataframe(df: pd.DataFrame, download_col: str = "download") -> List[Tuple[int, str]]:
    """DataFrameì—ì„œ download ì»¬ëŸ¼ì´ ë‚ ì§œê°€ ì•„ë‹Œ í–‰ì˜ URL ì¶”ì¶œ
    
    Args:
        df: Excelì—ì„œ ì¶”ì¶œí•œ DataFrame
        download_col: download ì»¬ëŸ¼ ì´ë¦„
    
    Returns:
        [(í–‰ ì¸ë±ìŠ¤, URL), ...] ë¦¬ìŠ¤íŠ¸
    """
    url_list = []
    
    if download_col not in df.columns:
        print(f"âš ï¸  '{download_col}' ì»¬ëŸ¼ì´ DataFrameì— ì—†ìŠµë‹ˆë‹¤")
        return url_list
    
    for idx, row in df.iterrows():
        value = row[download_col]
        
        # ë‚ ì§œ í˜•ì‹ì´ ì•„ë‹ˆê³ , ë¬¸ìì—´ì´ê³ , httpë¡œ ì‹œì‘í•˜ë©´ URLë¡œ ê°„ì£¼
        if pd.notna(value) and isinstance(value, str):
            # ë‚ ì§œ íŒ¨í„´ ì²´í¬ (YYYY-MM-DD, YYYY/MM/DD ë“±)
            if not any(sep in value for sep in ['-', '/', '.']):
                # http ë˜ëŠ” httpsë¡œ ì‹œì‘í•˜ë©´ URL
                if value.startswith(('http://', 'https://')):
                    url_list.append((idx, value))
    
    print(f"âœ… {len(url_list)}ê°œì˜ URL ì¶”ì¶œ ì™„ë£Œ")
    return url_list


def process_crawling(
    urls: List[Tuple[int, str]], 
    crawl_config: dict, 
    output_dir: Path
) -> Dict[int, dict]:
    """crawl_utilsì™€ Firefox WebDriverë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ ìˆ˜í–‰
    
    Args:
        urls: [(í–‰ ì¸ë±ìŠ¤, URL), ...] ë¦¬ìŠ¤íŠ¸
        crawl_config: xlcrawl.yamlì˜ xlcrawl ì„¹ì…˜
        output_dir: ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
    
    Returns:
        {í–‰ ì¸ë±ìŠ¤: í¬ë¡¤ë§ ê²°ê³¼} ë”•ì…”ë„ˆë¦¬
    """
    from image_utils import ImageDownloader, ImageDownloadPolicy
    
    results = {}
    
    try:
        # Firefox WebDriver ì„¤ì • ë¡œë“œ
        firefox_config = crawl_config.get("firefox", {})
        
        print(f"ğŸ”§ Firefox WebDriver ì´ˆê¸°í™” ì¤‘...")
        print(f"  - Headless: {firefox_config.get('headless', False)}")
        print(f"  - Window Size: {firefox_config.get('window_size', [1440, 900])}")
        print(f"  - Session Path: {firefox_config.get('session_path')}")
        
        # WebDriver ìƒì„± (dictë¡œ ì„¤ì • ì „ë‹¬)
        driver = create_webdriver("firefox", firefox_config)
        
        # ImageDownloader ìƒì„±
        download_policy = ImageDownloadPolicy(
            timeout=crawl_config.get("crawl", {}).get("timeout", 30),
            max_retries=crawl_config.get("crawl", {}).get("retry", 3),
            user_agent=crawl_config.get("crawl", {}).get("user_agent", "Mozilla/5.0")
        )
        downloader = ImageDownloader(download_policy)
        
        # JS snippet ë¡œë“œ
        extract_config = crawl_config.get("extract", {})
        js_snippet = extract_config.get("js_snippet", "return {};")
        
        print(f"  - JS Extract: {'âœ… ì„¤ì •ë¨' if js_snippet else 'âŒ ë¯¸ì„¤ì •'}")
        print(f"  - Image Downloader: âœ… ì¤€ë¹„ ì™„ë£Œ")
        
        # ê° URL í¬ë¡¤ë§
        for idx, url in urls:
            print(f"\nğŸŒ í¬ë¡¤ë§ ì¤‘ [{idx}]: {url}")
            
            try:
                # í˜ì´ì§€ ë¡œë“œ
                driver.driver.get(url)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ê°„ë‹¨í•œ sleep)
                import time
                time.sleep(2)  # TODO: WebDriverWait ì‚¬ìš©
                
                # JS extract ì‹¤í–‰
                data = driver.driver.execute_script(js_snippet)
                
                if not isinstance(data, dict):
                    print(f"  âš ï¸  JS extract ê²°ê³¼ê°€ dictê°€ ì•„ë‹˜: {type(data)}")
                    data = {}
                
                # ì¶”ì¶œëœ ë°ì´í„° í™•ì¸
                title = data.get("title", "")
                price = data.get("price", "")
                images = data.get("images", [])
                
                print(f"  ğŸ“ Title: {title[:50]}..." if title else "  ğŸ“ Title: (ì—†ìŒ)")
                print(f"  ğŸ’° Price: {price}")
                print(f"  ğŸ–¼ï¸  Images: {len(images)}ê°œ")
                
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                downloaded_images = []
                if images:
                    save_dir = output_dir / f"product_{idx}"
                    download_results = downloader.download_many(
                        images,
                        save_dir,
                        prefix=f"product_{idx}"
                    )
                    
                    # ì„±ê³µí•œ ì´ë¯¸ì§€ë§Œ ìˆ˜ì§‘
                    downloaded_images = [
                        r["path"] for r in download_results 
                        if r["status"] == "success"
                    ]
                
                # ê²°ê³¼ ì €ì¥
                results[idx] = {
                    "url": url,
                    "status": "success",
                    "timestamp": datetime.now().isoformat(),
                    "title": title,
                    "price": price,
                    "images_count": len(downloaded_images),
                    "images_total": len(images),
                    "images": [str(p) for p in downloaded_images],
                    "data": data  # ì „ì²´ ì¶”ì¶œ ë°ì´í„°
                }
                
                print(f"  âœ… ì„±ê³µ: {len(downloaded_images)}/{len(images)}ê°œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ")
                
            except Exception as e:
                print(f"  âŒ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                
                results[idx] = {
                    "url": url,
                    "status": "failed",
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
        
        # WebDriver ì¢…ë£Œ
        driver.quit()
        downloader.close()
        print(f"\nâœ… Firefox WebDriver ì¢…ë£Œ")
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    return results


def update_download_column(
    xl_controller: XlController, 
    df: pd.DataFrame,
    download_col: str,
    crawl_results: Dict[int, dict]
) -> None:
    """í¬ë¡¤ë§ ì„±ê³µí•œ í–‰ì˜ download ì»¬ëŸ¼ì— ë‚ ì§œ ê¸°ì…
    
    Args:
        xl_controller: XlController ì¸ìŠ¤í„´ìŠ¤
        df: ì›ë³¸ DataFrame
        download_col: download ì»¬ëŸ¼ ì´ë¦„
        crawl_results: í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    ws = xl_controller.get_worksheet()
    today = datetime.now().strftime("%Y-%m-%d")
    
    # download ì»¬ëŸ¼ì˜ Excel ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
    col_idx = df.columns.get_loc(download_col) + 1  # Excelì€ 1-based
    
    updated_count = 0
    
    for row_idx, result in crawl_results.items():
        if result.get("status") == "success":
            # DataFrameì˜ row_idxë¥¼ Excel í–‰ ë²ˆí˜¸ë¡œ ë³€í™˜ (í—¤ë” ê³ ë ¤)
            excel_row = row_idx + 2  # 1-based + header row
            
            # ë‚ ì§œ ê¸°ì…
            ws.write_cell(excel_row, col_idx, today)
            updated_count += 1
    
    print(f"âœ… {updated_count}ê°œ í–‰ì˜ download ì»¬ëŸ¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ")


def main():
    """ë©”ì¸ ì›Œí¬í”Œë¡œìš°"""
    print("=" * 80)
    print("Excel í¬ë¡¤ë§ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    print("=" * 80)
    
    try:
        # Step 1: CASHOP_PATHS í™˜ê²½ë³€ìˆ˜ì—ì„œ paths.local.yaml ë¡œë“œ
        print("\n[Step 1] paths.local.yaml ë¡œë“œ")
        paths_config = load_paths_config()
        
        # Step 2: paths.local.yamlì—ì„œ excel.yaml, xlcrawl.yaml ê²½ë¡œ ì¶”ì¶œ
        print("\n[Step 2] Excel ë° Crawl ì„¤ì • íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ")
        excel_config_path, xlcrawl_config_path = get_config_paths(paths_config)
        
        # Step 3: xlcrawl.yaml ë¡œë“œ (paths_configë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©)
        print("\n[Step 3] xlcrawl.yaml ì„¤ì • ë¡œë“œ")
        yaml_policy = BaseParserPolicy(
            file_path=str(xlcrawl_config_path),
            enable_placeholder=True,
            enable_reference=True,
            enable_env=False,
            enable_include=False,
            default_section=None,
            encoding="utf-8",
            on_error="raise",
            safe_mode=True
        )
        crawl_loader = ConfigLoader(xlcrawl_config_path, yaml_policy=yaml_policy)
        # paths_configë¥¼ ë¨¼ì € ë¡œë“œí•œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        crawl_config = crawl_loader.as_dict().get("xlcrawl", {})
        
        download_col = crawl_config.get("download_column", "download")
        output_dir = Path(crawl_config.get("image_save_dir", "output/images"))
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"  - Download ì»¬ëŸ¼: {download_col}")
        print(f"  - ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ: {output_dir}")
        
        # Step 4: xl_utilsë¡œ Excel íŒŒì¼ ì—´ê¸°
        print("\n[Step 4] Excel íŒŒì¼ ì—´ê¸°")
        
        # paths_configì—ì„œ ì§ì ‘ Excel íŒŒì¼ ê²½ë¡œì™€ ì‹œíŠ¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        root_dir = paths_config.get("root", "M:/CALife/CAShop - êµ¬ë§¤ëŒ€í–‰")
        excel_file_path = f"{root_dir}/01.All Product List.xlsx"
        sheet_name = paths_config.get("all_product_xl_file_sheet", "Purchase")
        
        print(f"  - Excel íŒŒì¼: {excel_file_path}")
        print(f"  - ì‹œíŠ¸ ì´ë¦„: {sheet_name}")
        
        # XlControllerì— ì§ì ‘ ê²½ë¡œ ì „ë‹¬ (runtime override)
        with XlController(excel_config_path) as xl:
            ws = xl.get_worksheet(excel_path=excel_file_path, sheet_name=sheet_name)
            
            # Step 5: DataFrame ì¶”ì¶œ
            print("\n[Step 5] DataFrame ì¶”ì¶œ")
            df = ws.to_dataframe()
            print(f"  - DataFrame í¬ê¸°: {df.shape}")
            print(f"  - ì»¬ëŸ¼: {list(df.columns)}")
            
            # Step 6: download ì»¬ëŸ¼ì—ì„œ URL ì¶”ì¶œ
            print("\n[Step 6] URL ì¶”ì¶œ")
            urls = extract_urls_from_dataframe(df, download_col)
            
            if not urls:
                print("âš ï¸  í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # Step 7: í¬ë¡¤ë§ ìˆ˜í–‰
            print("\n[Step 7] í¬ë¡¤ë§ ìˆ˜í–‰")
            crawl_results = process_crawling(urls, crawl_config, output_dir)
            
            # Step 8: download ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
            print("\n[Step 8] download ì»¬ëŸ¼ ì—…ë°ì´íŠ¸")
            update_download_column(xl, df, download_col, crawl_results)
        
        print("\n" + "=" * 80)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
